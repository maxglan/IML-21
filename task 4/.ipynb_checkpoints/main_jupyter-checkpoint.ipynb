{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat May 22 17:42:45 2021\n",
    "\n",
    "@author: georgengin\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "\n",
    "# import efficientnet.keras as efn\n",
    "\n",
    "\n",
    "#importing the txt with the image names\n",
    "train_triplets = np.genfromtxt(\"train_triplets.txt\", dtype='str')\n",
    "test_triplets = np.genfromtxt(\"test_triplets.txt\", dtype='str')\n",
    "\n",
    "\"\"\" 1) functions for preprocessing images into format required by NN \"\"\"\n",
    "\n",
    "#required size for EfficientNetB0\n",
    "target_shape = (224,224)\n",
    "\n",
    "def preprocess_image(filename):\n",
    "    \"\"\"\n",
    "    Load the specified file as a JPG image, preprocess it and resize it to the target shape.\n",
    "    \"\"\"\n",
    "\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, target_shape)\n",
    "    return image\n",
    "    # return image[None]\n",
    "\n",
    "def preprocess_image_N(filename):\n",
    "    \"\"\"\n",
    "    Load the specified file as a JPG image, preprocess it and resize it to the target shape.\n",
    "    \"\"\"\n",
    "\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, target_shape)\n",
    "    return image[None]\n",
    "    \n",
    "def preprocess_triplets(anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and preprocess them.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative),\n",
    "    )\n",
    "\n",
    "def preprocess_triplets_N(anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and preprocess them.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        preprocess_image_N(anchor),\n",
    "        preprocess_image_N(positive),\n",
    "        preprocess_image_N(negative),\n",
    "    )\n",
    "\n",
    "\"\"\" 1.5) importing images into tf.Dataset\"\"\"\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "anchor_images = list(\n",
    "    [str(cwd + \"/food/\" + f +\".jpg\") for f in train_triplets[:,0]])\n",
    "\n",
    "positive_images = sorted(\n",
    "    [str(cwd + \"/food/\" + f +\".jpg\") for f in train_triplets[:,1]])\n",
    "\n",
    "negative_images = sorted(\n",
    "    [str(cwd + \"/food/\" + f +\".jpg\") for f in train_triplets[:,2]])\n",
    "\n",
    "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "dataset = dataset.map(preprocess_triplets)\n",
    "\n",
    "#split into validation set\n",
    "image_count = len(anchor_images)\n",
    "image_round = (round(image_count * 0.8), image_count - round(image_count * 0.8)) #= (47612, 11903)\n",
    "\n",
    "train_dataset = dataset.take(round(image_count * 0.8))\n",
    "val_dataset = dataset.skip(round(image_count * 0.8))\n",
    "\n",
    "train_dataset = train_dataset.batch(100)\n",
    "# train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "val_dataset = val_dataset.batch(100)\n",
    "# val_dataset = val_dataset.prefetch(8)\n",
    "\n",
    "#repeat for test data\n",
    "anchor_test = list(\n",
    "    [str(cwd + \"/food/\" + f +\".jpg\") for f in test_triplets[:,0]])\n",
    "\n",
    "positive_test = sorted(\n",
    "    [str(cwd + \"/food/\" + f +\".jpg\") for f in test_triplets[:,1]])\n",
    "\n",
    "negative_test = sorted(\n",
    "    [str(cwd + \"/food/\" + f +\".jpg\") for f in test_triplets[:,2]])\n",
    "\n",
    "anchor_test = tf.data.Dataset.from_tensor_slices(anchor_test)\n",
    "positive_test = tf.data.Dataset.from_tensor_slices(positive_test)\n",
    "negative_test = tf.data.Dataset.from_tensor_slices(negative_test)\n",
    "\n",
    "dataset_test = tf.data.Dataset.zip((anchor_test, positive_test, negative_test))\n",
    "dataset_test = dataset_test.map(preprocess_triplets_N)\n",
    "\n",
    "#dataset_test is <MapDataset shapes: ((224, 224, 3), (224, 224, 3), (224, 224, 3)), types: (tf.float32, tf.float32, tf.float32)>\n",
    "\n",
    "\"\"\" 2) Setting up the embedding generator model \"\"\"\n",
    "#size of output layer\n",
    "emb_size = 8\n",
    "\n",
    "base_model = EfficientNetB0(input_shape = target_shape + (3,), include_top = False, weights = 'imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "pool = layers.MaxPool2D(pool_size=(7,7))\n",
    "pool_layer = pool(base_model.output)\n",
    "flatten = layers.Flatten()(pool_layer)\n",
    "dense1 = layers.Dense(32, activation=\"relu\")(flatten)\n",
    "output = layers.Dense(emb_size)(dense1)\n",
    "\n",
    "embedding = Model(base_model.input, output, name=\"Embedding\")\n",
    "# embedding.summary()\n",
    "\n",
    "\"\"\"3) creating the Siamese Network\"\"\"\n",
    "input_anchor = layers.Input(target_shape + (3,))\n",
    "input_positive = layers.Input(target_shape + (3,))\n",
    "input_negative = layers.Input(target_shape + (3,))\n",
    "\n",
    "embedding_anchor = embedding(input_anchor)\n",
    "embedding_positive = embedding(input_positive)\n",
    "embedding_negative = embedding(input_negative)\n",
    "\n",
    "output_emb = layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "siam_model = tf.keras.models.Model(inputs = [input_anchor, input_positive, input_negative], \n",
    "                                   outputs = output_emb)\n",
    "\n",
    "\"\"\"\n",
    "siam_model asks for input: (<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_2')>, \n",
    "          <KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_3')>, \n",
    "          <KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_4')>)\n",
    "\n",
    "siam_model has output: # KerasTensor(type_spec=TensorSpec(shape=(None, 24), dtype=tf.float32, name=None)\n",
    "\"\"\"\n",
    "# siam_model.summary()\n",
    "\n",
    "#defining the triplet loss\n",
    "margin = 0.2\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:2*emb_size], y_pred[:,2*emb_size:]\n",
    "    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)\n",
    "    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)\n",
    "    return tf.maximum(positive_dist - negative_dist + margin, 0.)\n",
    "\n",
    "\"\"\"3.5) create dummy y values, see model.fit documentation\"\"\"\n",
    "train_dummy_np = np.zeros((image_round[0], emb_size * 3))\n",
    "val_dummy_np = np.zeros((image_round[1], emb_size * 3))\n",
    "\n",
    "train_dummy = tf.data.Dataset.from_tensor_slices(train_dummy_np)\n",
    "train_dummy = train_dummy.batch(100)\n",
    "val_dummy = tf.data.Dataset.from_tensor_slices(train_dummy_np)\n",
    "val_dummy = val_dummy.batch(100)\n",
    "\n",
    "\"\"\"4) fitting model\"\"\"\n",
    "siam_model.compile(loss = triplet_loss, optimizer ='adam')\n",
    "\n",
    "input_fit = tf.data.Dataset.zip((train_dataset, train_dummy))\n",
    "val_fit = tf.data.Dataset.zip((val_dataset, val_dummy))\n",
    "\n",
    "siam_model.fit(input_fit, epochs=10, validation_data = val_fit, verbose=1)\n",
    "# siam_model.fit(train_dataset, epochs=1, validation_data = val_dataset, verbose=1)\n",
    "\n",
    "\n",
    "\"\"\"5) applying on test set\"\"\"\n",
    "test_dummy_np = np.zeros((image_count, emb_size * 3))\n",
    "test_dummy = tf.data.Dataset.from_tensor_slices(test_dummy_np)\n",
    "test_fit = tf.data.Dataset.zip((dataset_test, test_dummy))\n",
    "\n",
    "predicted_vectors = siam_model.predict(test_fit)\n",
    "\n",
    "#create ourput file\n",
    "def choose(prediction):\n",
    "    length = len(predicted_vectors[0,:])\n",
    "    result = np.zeros(length, dtype = int)\n",
    "    for i in range(length):\n",
    "        anchor, positive, negative = prediction[i,:emb_size], prediction[i,emb_size:2*emb_size], prediction[i,2*emb_size:]\n",
    "        positive_dist = np.linalg.norm(anchor - positive)\n",
    "        negative_dist = np.linalg.norm(anchor - negative)\n",
    "        if positive_dist > negative_dist:\n",
    "            result[i] = 1\n",
    "    return result\n",
    "\n",
    "result = choose(predicted_vectors)\n",
    "np.savetxt(\"stupid_result.txt\", result, fmt='%1.0i')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
